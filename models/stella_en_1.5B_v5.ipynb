{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc2f9d-7c8b-4953-9f0a-3e3d8939d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.std import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791f28c-e121-4099-aca4-3aa0c9f208ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Using {num_gpus} GPUs: {[torch.cuda.get_device_name(i) for i in range(num_gpus)]}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    num_gpus = 0\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2e798-e8e8-43bd-b66c-572a117fd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\", trust_remote_code=True)\n",
    "model.max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5ed90-e4f3-4e2e-9518-505b78cce5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_gpus > 1:\n",
    "    model = torch.nn.DataParallel(model) \n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a430a-0498-4453-83ac-144b4ce0fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../document_ranking_input_true_data/document_ranking_query.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009addf-1f6b-485d-ba1c-a0f8d0f35fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d8cb3-06dc-403a-996c-10a8c49b0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(query, documents):\n",
    "    \"\"\"Calculate the matching score of a single query for multiple documents\"\"\"\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.encode([query], prompt_name=\"s2p_query\", convert_to_tensor=True, normalize_embeddings=True)  # (1, dim)\n",
    "        document_embeddings = model.encode(documents, convert_to_tensor=True, normalize_embeddings=True)  # (100, dim)\n",
    "        # scores = (query_embedding @ document_embeddings.T) * 100  # (1, 100) -> (100,)\n",
    "        scores = model.similarity(query_embedding, document_embeddings)\n",
    "        \n",
    "        del query_embedding, document_embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "    return scores.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258b2c1-4699-47bf-b6dc-c28fd659abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "qids, pids, ranked_pids, ranked_scores = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f607a-e8fa-4687-a2f4-348870a689f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing of data\n",
    "batch_size = 100\n",
    "num_samples = len(data)\n",
    "\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    temp_df = data.iloc[i:i+batch_size]\n",
    "    \n",
    "    # Since the 100 queries are the same, only encode the first one\n",
    "    query = temp_df[\"query\"].iloc[0]\n",
    "    passages = temp_df[\"passage\"].tolist()\n",
    "    query_id = temp_df[\"qid\"].iloc[0]  # The 100 qid values are the same, take the first one\n",
    "    passage_ids = temp_df[\"docid\"].to_numpy()  # Convert to a NumPy array\n",
    "\n",
    "    scores = get_score(query, passages)  # Compute scores (100,)\n",
    "\n",
    "    # Use NumPy sorting for faster processing\n",
    "    sorted_indices = np.argsort(-scores)  # Descending order sorting indices\n",
    "    sorted_pids_batch = passage_ids[sorted_indices]  # Sorted PIDs\n",
    "    sorted_scores_batch = scores[sorted_indices]  # Sorted scores\n",
    "\n",
    "    # Store results\n",
    "    qids.extend([query_id] * batch_size)  # 100 identical query_id values\n",
    "    pids.extend(passage_ids)  # Original passage_id values\n",
    "    ranked_pids.extend(sorted_pids_batch)  # Sorted PIDs\n",
    "    ranked_scores.extend(sorted_scores_batch)  # Sorted scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2ee15-feca-41b9-be56-e217a5b4b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"qid\":qids, \"docid\":pids, \"ranked_docid\":ranked_pids, \"scores\":ranked_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9603cb-7993-4788-a6ca-91f5a734e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../passage_output_result/stella_result.tsv\",sep=\"\\t\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-jupyter-citation",
   "language": "python",
   "name": "my-jupyter-citation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
