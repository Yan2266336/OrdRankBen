{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0cf70-0c3a-41fd-9756-4983fe6c0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.std import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec7916-e721-4f22-84d0-728552eecebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b439ed-4d5e-4149-8e27-0cc7fb083e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Soyoung97/RankT5-base\"\n",
    "data_path = \"../passage_ranking_input_true_data/passage_ranking_query.tsv\"\n",
    "output_path = \"../passage_output_result/rankT5_result.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931bf38-3c2a-4e55-8922-f7d8f50f0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(data_path,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1d0f0-0b95-4f5a-9bf0-89065b52d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9aa881-c21e-4cbe-b91d-bd279088b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dataset = dataset.groupby(\"qid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff507c-3518-460f-b32d-3b7bdc76d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "for qid in tqdm(group_dataset.groups.keys()):\n",
    "    temp_df = group_dataset.get_group(qid)\n",
    "    docid_list = temp_df.pid.tolist()\n",
    "    query = temp_df['query'].tolist()[0]\n",
    "    passage_list = temp_df.passage.tolist()\n",
    "\n",
    "    input_data.append({\"qid\": qid, \"query\": query, \"pids\":docid_list, \"passages\":passage_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95babcd9-8269-41df-8c15-deb59992b381",
   "metadata": {},
   "source": [
    "## loading tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00377628-6b89-4c5d-8166-e6d0ad7cc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2890f20-a791-45b8-ae50-c8abb4e3322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(query, passage):\n",
    "    return f\"query: {query} passage: {passage}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69efad89-a261-4cb7-a48e-7debed299147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_passages(query, passages, docids, batch_size=16):\n",
    "    scores = []\n",
    "    formatted_inputs = [format_input(query, passage) for passage in passages]\n",
    "    \n",
    "    # Process tokenization and inference in batches\n",
    "    for i in range(0, len(formatted_inputs), batch_size):\n",
    "        batch_inputs = formatted_inputs[i:i + batch_size]\n",
    "        batch_docids = docids[i:i + batch_size]\n",
    "        \n",
    "        # Batch tokenization\n",
    "        inputs = tokenizer(batch_inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Set decoder_input_ids to <pad> token, suitable for batching\n",
    "        decoder_input_ids = torch.full(\n",
    "            (len(batch_inputs), 1), tokenizer.pad_token_id, dtype=torch.long, device=device\n",
    "        )\n",
    "        \n",
    "        # Perform batch inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"], \n",
    "                            attention_mask=inputs[\"attention_mask\"], \n",
    "                            decoder_input_ids=decoder_input_ids)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        # Compute the score for each passage\n",
    "        batch_scores = logits[:, 0].mean(dim=-1).tolist()\n",
    "        \n",
    "        # Store docid along with the corresponding score\n",
    "        scores.extend(zip(batch_docids, batch_scores))\n",
    "\n",
    "    # Sort based on scores\n",
    "    ranked_passages = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract sorted docids, passages, and scores\n",
    "    ranked_docids = [item[0] for item in ranked_passages]\n",
    "    ranked_scores = [item[1] for item in ranked_passages]\n",
    "\n",
    "    return ranked_docids, ranked_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27854058-2d46-459e-9b94-5f8c6733f79b",
   "metadata": {},
   "source": [
    "## runing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7d184-40fb-448e-b4e4-6bf028a83450",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = []\n",
    "chunk_size = 100\n",
    "for input_ in tqdm(input_data):\n",
    "    query = input_['query']\n",
    "    qid =  input_['qid']\n",
    "    docids = input_['pids']\n",
    "    passages_list = input_['passages']\n",
    "\n",
    "    ranked_docids, ranked_scores = rank_passages(query, passages_list, docids)\n",
    "    final_result.append({\"qid\":qid, \"pid\":docids, \"ranked_pid\":ranked_docids, \"scores\":ranked_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04557de-2169-4fa7-b1df-e8d36f2954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(final_result)\n",
    "result_df = result_df.explode([\"pid\", \"ranked_pid\", \"scores\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08738efc-4341-4e6c-988c-2b12e2bc340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(output_path,sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe9145c-3133-431b-b942-06de7d5202b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-jupyter-citation",
   "language": "python",
   "name": "my-jupyter-citation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
