{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb31a0f-95e4-4371-b78d-48faa4c200b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.std import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd9416-ce96-4967-a4fa-6cf6dedd4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af5835-cf2c-4994-8055-7d73068ca522",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", device, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a882a-c09d-40b0-bbd0-95e549f7f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.max_seq_length=512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765d9e3-7af6-4a1a-8dc5-f5f75ee99024",
   "metadata": {},
   "source": [
    "## loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7e694-5414-4e12-8989-7ebc3ed70126",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../document_ranking_input_true_data/document_ranking_query.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d7195-5662-4f4c-a743-497cb0444dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d69299-1f3c-4e30-856c-cee704653a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(query, documents):\n",
    "    \"\"\"Calculate the matching score of a single query for multiple documents\"\"\"\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.encode([query], prompt_name=\"query\", convert_to_tensor=True, normalize_embeddings=True)  # (1, dim)\n",
    "        document_embeddings = model.encode(documents, convert_to_tensor=True, normalize_embeddings=True)  # (100, dim)\n",
    "        # scores = (query_embedding @ document_embeddings.T) * 100  # (1, 100) -> (100,)\n",
    "        scores = model.similarity(query_embedding, document_embeddings) \n",
    "        \n",
    "        del query_embedding, document_embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "    return scores.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d9f43-cba1-45fc-a4f4-33b06ae82ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qids, pids, ranked_pids, ranked_scores = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a225b6-9ee0-4be8-94f7-dcf2c6a2325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing data\n",
    "batch_size = 100\n",
    "num_samples = len(data)\n",
    "\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    temp_df = data.iloc[i:i+batch_size]\n",
    "    \n",
    "    # Since the 100 queries are the same, only the first one is encoded\n",
    "    query = temp_df[\"query\"].iloc[0]\n",
    "    passages = temp_df[\"passage\"].tolist()\n",
    "    query_id = temp_df[\"qid\"].iloc[0]  # 100 qids are the same, take the first one\n",
    "    passage_ids = temp_df[\"docid\"].to_numpy() \n",
    "\n",
    "    scores = get_score(query, passages)  \n",
    "\n",
    "    # NumPy sorting, speed up\n",
    "    sorted_indices = np.argsort(-scores)  # Sort index in descending order\n",
    "    sorted_pids_batch = passage_ids[sorted_indices]  \n",
    "    sorted_scores_batch = scores[sorted_indices]  \n",
    "\n",
    "    \n",
    "    qids.extend([query_id] * batch_size)  \n",
    "    pids.extend(passage_ids)  \n",
    "    ranked_pids.extend(sorted_pids_batch)  \n",
    "    ranked_scores.extend(sorted_scores_batch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b66f7-2d00-402b-9454-6bc2b225c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"qid\":qids, \"pid\":pids, \"ranked_pid\":ranked_pids, \"scores\":ranked_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffa73c-6609-47a7-871c-955bf4069ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../passage_output_result/qwen2_result.tsv\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90acab7-4956-403b-8712-e30e8427a837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-jupyter-citation",
   "language": "python",
   "name": "my-jupyter-citation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
